{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpeechRecognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolaerosca/colab_notebooks/blob/master/SpeechRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "B13lf2q45vvF",
        "colab_type": "code",
        "outputId": "5f64323b-453a-48e4-866e-b6ae781b6b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2057
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "# platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "# cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "# accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "# !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "# !pip update torch\n",
        "# !pip install torch==1.0 torchvision\n",
        "import torch as t\n",
        "from torch.nn import Parameter\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "from tempfile import NamedTemporaryFile\n",
        "\n",
        "from torch.distributed import get_rank\n",
        "from torch.distributed import get_world_size\n",
        "from torch.utils.data.sampler import Sampler\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "import torch\n",
        "\n",
        "# !pip install torchaudio\n",
        "# import torchaudio\n",
        "import math\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "!sudo apt-get install sox libsox-dev libsox-fmt-all"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libao-common libao4 libid3tag0 libmad0 libmagic-mgc libmagic1\n",
            "  libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa libsox-fmt-ao\n",
            "  libsox-fmt-base libsox-fmt-mp3 libsox-fmt-oss libsox-fmt-pulse libsox3\n",
            "Suggested packages:\n",
            "  libaudio2 file\n",
            "The following NEW packages will be installed:\n",
            "  libao-common libao4 libid3tag0 libmad0 libmagic-mgc libmagic1\n",
            "  libopencore-amrnb0 libopencore-amrwb0 libsox-dev libsox-fmt-all\n",
            "  libsox-fmt-alsa libsox-fmt-ao libsox-fmt-base libsox-fmt-mp3 libsox-fmt-oss\n",
            "  libsox-fmt-pulse libsox3 sox\n",
            "0 upgraded, 18 newly installed, 0 to remove and 3 not upgraded.\n",
            "Need to get 1,267 kB of archives.\n",
            "After this operation, 9,144 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopencore-amrnb0 amd64 0.1.3-2.1 [92.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopencore-amrwb0 amd64 0.1.3-2.1 [45.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.1 [184 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.1 [68.4 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libao-common all 1.2.2+20180113-1ubuntu1 [6,644 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libao4 amd64 1.2.2+20180113-1ubuntu1 [35.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libid3tag0 amd64 0.15.1b-13 [31.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libmad0 amd64 0.15.1b-9ubuntu18.04.1 [64.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsox3 amd64 14.4.2-3 [225 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsox-fmt-alsa amd64 14.4.2-3 [10.6 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsox-fmt-ao amd64 14.4.2-3 [7,452 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsox-fmt-base amd64 14.4.2-3 [32.0 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsox-fmt-mp3 amd64 14.4.2-3 [15.8 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsox-fmt-oss amd64 14.4.2-3 [9,004 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsox-fmt-pulse amd64 14.4.2-3 [7,372 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsox-fmt-all amd64 14.4.2-3 [5,116 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsox-dev amd64 14.4.2-3 [325 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/universe amd64 sox amd64 14.4.2-3 [101 kB]\n",
            "Fetched 1,267 kB in 1s (1,254 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 18.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "(Reading database ... 111313 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libopencore-amrnb0_0.1.3-2.1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../01-libopencore-amrwb0_0.1.3-2.1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "Preparing to unpack .../02-libmagic-mgc_1%3a5.32-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.1) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../03-libmagic1_1%3a5.32-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.1) ...\n",
            "Selecting previously unselected package libao-common.\n",
            "Preparing to unpack .../04-libao-common_1.2.2+20180113-1ubuntu1_all.deb ...\n",
            "Unpacking libao-common (1.2.2+20180113-1ubuntu1) ...\n",
            "Selecting previously unselected package libao4:amd64.\n",
            "Preparing to unpack .../05-libao4_1.2.2+20180113-1ubuntu1_amd64.deb ...\n",
            "Unpacking libao4:amd64 (1.2.2+20180113-1ubuntu1) ...\n",
            "Selecting previously unselected package libid3tag0:amd64.\n",
            "Preparing to unpack .../06-libid3tag0_0.15.1b-13_amd64.deb ...\n",
            "Unpacking libid3tag0:amd64 (0.15.1b-13) ...\n",
            "Selecting previously unselected package libmad0:amd64.\n",
            "Preparing to unpack .../07-libmad0_0.15.1b-9ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking libmad0:amd64 (0.15.1b-9ubuntu18.04.1) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../08-libsox3_14.4.2-3_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2-3) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../09-libsox-fmt-alsa_14.4.2-3_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2-3) ...\n",
            "Selecting previously unselected package libsox-fmt-ao:amd64.\n",
            "Preparing to unpack .../10-libsox-fmt-ao_14.4.2-3_amd64.deb ...\n",
            "Unpacking libsox-fmt-ao:amd64 (14.4.2-3) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../11-libsox-fmt-base_14.4.2-3_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2-3) ...\n",
            "Selecting previously unselected package libsox-fmt-mp3:amd64.\n",
            "Preparing to unpack .../12-libsox-fmt-mp3_14.4.2-3_amd64.deb ...\n",
            "Unpacking libsox-fmt-mp3:amd64 (14.4.2-3) ...\n",
            "Selecting previously unselected package libsox-fmt-oss:amd64.\n",
            "Preparing to unpack .../13-libsox-fmt-oss_14.4.2-3_amd64.deb ...\n",
            "Unpacking libsox-fmt-oss:amd64 (14.4.2-3) ...\n",
            "Selecting previously unselected package libsox-fmt-pulse:amd64.\n",
            "Preparing to unpack .../14-libsox-fmt-pulse_14.4.2-3_amd64.deb ...\n",
            "Unpacking libsox-fmt-pulse:amd64 (14.4.2-3) ...\n",
            "Selecting previously unselected package libsox-fmt-all:amd64.\n",
            "Preparing to unpack .../15-libsox-fmt-all_14.4.2-3_amd64.deb ...\n",
            "Unpacking libsox-fmt-all:amd64 (14.4.2-3) ...\n",
            "Selecting previously unselected package libsox-dev:amd64.\n",
            "Preparing to unpack .../16-libsox-dev_14.4.2-3_amd64.deb ...\n",
            "Unpacking libsox-dev:amd64 (14.4.2-3) ...\n",
            "Selecting previously unselected package sox.\n",
            "Preparing to unpack .../17-sox_14.4.2-3_amd64.deb ...\n",
            "Unpacking sox (14.4.2-3) ...\n",
            "Setting up libid3tag0:amd64 (0.15.1b-13) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Setting up libao-common (1.2.2+20180113-1ubuntu1) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.1) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up libmad0:amd64 (0.15.1b-9ubuntu18.04.1) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
            "Setting up libsox3:amd64 (14.4.2-3) ...\n",
            "Setting up libsox-fmt-mp3:amd64 (14.4.2-3) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2-3) ...\n",
            "Setting up libsox-fmt-pulse:amd64 (14.4.2-3) ...\n",
            "Setting up libao4:amd64 (1.2.2+20180113-1ubuntu1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2-3) ...\n",
            "Setting up libsox-fmt-oss:amd64 (14.4.2-3) ...\n",
            "Setting up libsox-fmt-ao:amd64 (14.4.2-3) ...\n",
            "Setting up sox (14.4.2-3) ...\n",
            "Setting up libsox-fmt-all:amd64 (14.4.2-3) ...\n",
            "Setting up libsox-dev:amd64 (14.4.2-3) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uWXw3vsbmsFU",
        "colab_type": "code",
        "outputId": "5e304650-86d4-4694-d467-f6b0d010fb22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "6lIA5leG6iL7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Deep speech implementation"
      ]
    },
    {
      "metadata": {
        "id": "RsV88fLF6gr0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SequenceWise(nn.Module):\n",
        "    def __init__(self, module):\n",
        "        \"\"\"\n",
        "        Collapses input of dim T*N*H to (T*N)*H, and applies to a module.\n",
        "        Allows handling of variable sequence lengths and minibatch sizes.\n",
        "        :param module: Module to apply input to.\n",
        "        \"\"\"\n",
        "        super(SequenceWise, self).__init__()\n",
        "        self.module = module\n",
        "\n",
        "    def forward(self, x):\n",
        "        t, n = x.size(0), x.size(1)\n",
        "        x = x.view(t * n, -1)\n",
        "        x = self.module(x)\n",
        "        x = x.view(t, n, -1)\n",
        "        return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        tmpstr = self.__class__.__name__ + ' (\\n'\n",
        "        tmpstr += self.module.__repr__()\n",
        "        tmpstr += ')'\n",
        "        return tmpstr\n",
        "\n",
        "class MaskConv(nn.Module):\n",
        "    def __init__(self, seq_module):\n",
        "        \"\"\"\n",
        "        Adds padding to the output of the module based on the given lengths. This is to ensure that the\n",
        "        results of the model do not change when batch sizes change during inference.\n",
        "        Input needs to be in the shape of (BxCxDxT)\n",
        "        :param seq_module: The sequential module containing the conv stack.\n",
        "        \"\"\"\n",
        "        super(MaskConv, self).__init__()\n",
        "        self.seq_module = seq_module\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        \"\"\"\n",
        "        :param x: The input of size BxCxDxT\n",
        "        :param lengths: The actual length of each sequence in the batch\n",
        "        :return: Masked output from the module\n",
        "        \"\"\"\n",
        "        for module in self.seq_module:\n",
        "            x = module(x)\n",
        "            mask = torch.ByteTensor(x.size()).fill_(0)\n",
        "            if x.is_cuda:\n",
        "                mask = mask.cuda()\n",
        "            for i, length in enumerate(lengths):\n",
        "                length = length.item()\n",
        "                if (mask[i].size(2) - length) > 0:\n",
        "                    mask[i].narrow(2, length, mask[i].size(2) - length).fill_(1)\n",
        "            x = x.masked_fill(mask, 0)\n",
        "        return x, lengths\n",
        "\n",
        "      \n",
        "class InferenceBatchSoftmax(nn.Module):\n",
        "    def forward(self, input_):\n",
        "        if not self.training:\n",
        "            return F.softmax(input_, dim=-1)\n",
        "        else:\n",
        "            return input_\n",
        "\n",
        "\n",
        "class BatchRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, rnn_type=nn.LSTM, bidirectional=False, batch_norm=True):\n",
        "        super(BatchRNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "        self.batch_norm = SequenceWise(nn.BatchNorm1d(input_size)) if batch_norm else None\n",
        "        self.rnn = rnn_type(input_size=input_size, hidden_size=hidden_size,\n",
        "                            bidirectional=bidirectional, bias=True)\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "    def flatten_parameters(self):\n",
        "        self.rnn.flatten_parameters()\n",
        "\n",
        "    def forward(self, x, output_lengths):\n",
        "        if self.batch_norm is not None:\n",
        "            x = self.batch_norm(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x, output_lengths)\n",
        "        x, h = self.rnn(x)\n",
        "        x, _ = nn.utils.rnn.pad_packed_sequence(x)\n",
        "        if self.bidirectional:\n",
        "            x = x.view(x.size(0), x.size(1), 2, -1).sum(2).view(x.size(0), x.size(1), -1)  # (TxNxH*2) -> (TxNxH) by sum\n",
        "        return x\n",
        "\n",
        "class Lookahead(nn.Module):\n",
        "    # Wang et al 2016 - Lookahead Convolution Layer for Unidirectional Recurrent Neural Networks\n",
        "    # input shape - sequence, batch, feature - TxNxH\n",
        "    # output shape - same as input\n",
        "    def __init__(self, n_features, context):\n",
        "        # should we handle batch_first=True?\n",
        "        super(Lookahead, self).__init__()\n",
        "        self.n_features = n_features\n",
        "        self.weight = Parameter(torch.Tensor(n_features, context + 1))\n",
        "        assert context > 0\n",
        "        self.context = context\n",
        "        self.register_parameter('bias', None)\n",
        "        self.init_parameters()\n",
        "\n",
        "    def init_parameters(self):  # what's a better way initialiase this layer?\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input):\n",
        "        seq_len = input.size(0)\n",
        "        # pad the 0th dimension (T/sequence) with zeroes whose number = context\n",
        "        # Once pytorch's padding functions have settled, should move to those.\n",
        "        padding = torch.zeros(self.context, *(input.size()[1:])).type_as(input)\n",
        "        x = torch.cat((input, padding), 0)\n",
        "\n",
        "        # add lookahead windows (with context+1 width) as a fourth dimension\n",
        "        # for each seq-batch-feature combination\n",
        "        x = [x[i:i + self.context + 1] for i in range(seq_len)]  # TxLxNxH - sequence, context, batch, feature\n",
        "        x = torch.stack(x)\n",
        "        x = x.permute(0, 2, 3, 1)  # TxNxHxL - sequence, batch, feature, context\n",
        "\n",
        "        x = torch.mul(x, self.weight).sum(dim=3)\n",
        "        return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "               + 'n_features=' + str(self.n_features) \\\n",
        "               + ', context=' + str(self.context) + ')'\n",
        "\n",
        "\n",
        "class DeepSpeech(nn.Module):\n",
        "    def __init__(self, rnn_type=nn.LSTM, labels=\"abc\", rnn_hidden_size=768, nb_layers=5, audio_conf=None,\n",
        "                 bidirectional=True, context=20, mixed_precision=False):\n",
        "        super(DeepSpeech, self).__init__()\n",
        "\n",
        "        # model metadata needed for serialization/deserialization\n",
        "        if audio_conf is None:\n",
        "            audio_conf = {}\n",
        "        self._version = '0.0.1'\n",
        "        self._hidden_size = rnn_hidden_size\n",
        "        self._hidden_layers = nb_layers\n",
        "        self._rnn_type = rnn_type\n",
        "        self._audio_conf = audio_conf or {}\n",
        "        self._labels = labels\n",
        "        self._bidirectional = bidirectional\n",
        "        self.mixed_precision = mixed_precision\n",
        "        \n",
        "        sample_rate = self._audio_conf.get(\"sample_rate\", 16000)\n",
        "        window_size = self._audio_conf.get(\"window_size\", 0.02)\n",
        "        num_classes = len(self._labels)\n",
        "\n",
        "        self.conv = MaskConv(nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5)),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Hardtanh(0, 20, inplace=True),\n",
        "            nn.Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5)),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Hardtanh(0, 20, inplace=True)\n",
        "        ))\n",
        "        # Based on above convolutions and spectrogram size using conv formula (W - F + 2P)/ S+1\n",
        "        rnn_input_size = int(math.floor((sample_rate * window_size) / 2) + 1)\n",
        "        rnn_input_size = int(math.floor(rnn_input_size + 2 * 20 - 41) / 2 + 1)\n",
        "        rnn_input_size = int(math.floor(rnn_input_size + 2 * 10 - 21) / 2 + 1)\n",
        "        rnn_input_size *= 32\n",
        "\n",
        "        rnns = []\n",
        "        rnn = BatchRNN(input_size=rnn_input_size, hidden_size=rnn_hidden_size, rnn_type=rnn_type,\n",
        "                       bidirectional=bidirectional, batch_norm=False)\n",
        "        rnns.append(('0', rnn))\n",
        "        for x in range(nb_layers - 1):\n",
        "            rnn = BatchRNN(input_size=rnn_hidden_size, hidden_size=rnn_hidden_size, rnn_type=rnn_type,\n",
        "                           bidirectional=bidirectional)\n",
        "            rnns.append(('%d' % (x + 1), rnn))\n",
        "        self.rnns = nn.Sequential(OrderedDict(rnns))\n",
        "        self.lookahead = nn.Sequential(\n",
        "            # consider adding batch norm?\n",
        "            Lookahead(rnn_hidden_size, context=context),\n",
        "            nn.Hardtanh(0, 20, inplace=True)\n",
        "        ) if not bidirectional else None\n",
        "\n",
        "        fully_connected = nn.Sequential(\n",
        "            nn.BatchNorm1d(rnn_hidden_size),\n",
        "            nn.Linear(rnn_hidden_size, num_classes, bias=False)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            SequenceWise(fully_connected),\n",
        "        )\n",
        "        self.inference_softmax = InferenceBatchSoftmax()\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        if x.is_cuda and self.mixed_precision:\n",
        "          x = x.half()\n",
        "        lengths = lengths.cpu().int()\n",
        "        output_lengths = self.get_seq_lens(lengths)\n",
        "        x, _ = self.conv(x, output_lengths)\n",
        "\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # Collapse feature dimension\n",
        "        x = x.transpose(1, 2).transpose(0, 1).contiguous()  # TxNxH\n",
        "\n",
        "        for rnn in self.rnns:\n",
        "            x = rnn(x, output_lengths)\n",
        "\n",
        "        if not self._bidirectional:  # no need for lookahead layer in bidirectional\n",
        "            x = self.lookahead(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        x = x.transpose(0, 1)\n",
        "        # identity in training mode, softmax in eval mode\n",
        "        x = self.inference_softmax(x)\n",
        "        return x, output_lengths\n",
        "\n",
        "    def get_seq_lens(self, input_length):\n",
        "        \"\"\"\n",
        "        Given a 1D Tensor or Variable containing integer sequence lengths, return a 1D tensor or variable\n",
        "        containing the size sequences that will be output by the network.\n",
        "        :param input_length: 1D Tensor\n",
        "        :return: 1D Tensor scaled by model\n",
        "        \"\"\"\n",
        "        seq_len = input_length\n",
        "        for m in self.conv.modules():\n",
        "            if type(m) == nn.modules.conv.Conv2d:\n",
        "                seq_len = ((seq_len + 2 * m.padding[1] - m.dilation[1] * (m.kernel_size[1] - 1) - 1) / m.stride[1] + 1)\n",
        "        return seq_len.int()\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, path):\n",
        "        package = torch.load(path, map_location=lambda storage, loc: storage)\n",
        "        model = cls(rnn_hidden_size=package['hidden_size'], nb_layers=package['hidden_layers'],\n",
        "                    labels=package['labels'], audio_conf=package['audio_conf'],\n",
        "                    rnn_type=supported_rnns[package['rnn_type']], bidirectional=package.get('bidirectional', True))\n",
        "        model.load_state_dict(package['state_dict'])\n",
        "        for x in model.rnns:\n",
        "            x.flatten_parameters()\n",
        "        return model\n",
        "\n",
        "    @classmethod\n",
        "    def load_model_package(cls, package):\n",
        "        model = cls(rnn_hidden_size=package['hidden_size'], nb_layers=package['hidden_layers'],\n",
        "                    labels=package['labels'], audio_conf=package['audio_conf'],\n",
        "                    rnn_type=supported_rnns[package['rnn_type']], bidirectional=package.get('bidirectional', True))\n",
        "        model.load_state_dict(package['state_dict'])\n",
        "        return model\n",
        "\n",
        "    @staticmethod\n",
        "    def serialize(model, optimizer=None, epoch=None, iteration=None, loss_results=None,\n",
        "                  cer_results=None, wer_results=None, avg_loss=None, meta=None):\n",
        "        model = model.module if DeepSpeech.is_parallel(model) else model\n",
        "        package = {\n",
        "            'version': model._version,\n",
        "            'hidden_size': model._hidden_size,\n",
        "            'hidden_layers': model._hidden_layers,\n",
        "            'rnn_type': supported_rnns_inv.get(model._rnn_type, model._rnn_type.__name__.lower()),\n",
        "            'audio_conf': model._audio_conf,\n",
        "            'labels': model._labels,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'bidirectional': model._bidirectional\n",
        "        }\n",
        "        if optimizer is not None:\n",
        "            package['optim_dict'] = optimizer.state_dict()\n",
        "        if avg_loss is not None:\n",
        "            package['avg_loss'] = avg_loss\n",
        "        if epoch is not None:\n",
        "            package['epoch'] = epoch + 1  # increment for readability\n",
        "        if iteration is not None:\n",
        "            package['iteration'] = iteration\n",
        "        if loss_results is not None:\n",
        "            package['loss_results'] = loss_results\n",
        "            package['cer_results'] = cer_results\n",
        "            package['wer_results'] = wer_results\n",
        "        if meta is not None:\n",
        "            package['meta'] = meta\n",
        "        return package\n",
        "\n",
        "    @staticmethod\n",
        "    def get_labels(model):\n",
        "        return model.module._labels if model.is_parallel(model) else model._labels\n",
        "\n",
        "    @staticmethod\n",
        "    def get_param_size(model):\n",
        "        params = 0\n",
        "        for p in model.parameters():\n",
        "            tmp = 1\n",
        "            for x in p.size():\n",
        "                tmp *= x\n",
        "            params += tmp\n",
        "        return params\n",
        "\n",
        "    @staticmethod\n",
        "    def get_audio_conf(model):\n",
        "        return model.module._audio_conf if DeepSpeech.is_parallel(model) else model._audio_conf\n",
        "\n",
        "    @staticmethod\n",
        "    def get_meta(model):\n",
        "        m = model.module if DeepSpeech.is_parallel(model) else model\n",
        "        meta = {\n",
        "            \"version\": m._version,\n",
        "            \"hidden_size\": m._hidden_size,\n",
        "            \"hidden_layers\": m._hidden_layers,\n",
        "            \"rnn_type\": supported_rnns_inv[m._rnn_type]\n",
        "        }\n",
        "        return meta\n",
        "\n",
        "    @staticmethod\n",
        "    def is_parallel(model):\n",
        "        return isinstance(model, torch.nn.parallel.DataParallel) or \\\n",
        "               isinstance(model, torch.nn.parallel.DistributedDataParallel)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tHRKh4Qf801X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# download data\n",
        "!wget http://www.openslr.org/resources/12/dev-clean.tar.gz\n",
        "# !wget http://www.openslr.org/resources/12/dev-other.tar.gz\n",
        "\n",
        "!wget http://www.openslr.org/resources/12/test-clean.tar.gz\n",
        "# !wget http://www.openslr.org/resources/12/test-other.tar.gz\n",
        "\n",
        "!wget http://www.openslr.org/resources/12/train-clean-100.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J-VBHR1hdMHB",
        "colab_type": "code",
        "outputId": "6587a5cc-3aa5-4866-878e-bdd5aa045221",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install wget\n",
        "import os\n",
        "import wget\n",
        "import tarfile\n",
        "import argparse\n",
        "import subprocess\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import fnmatch\n",
        "\n",
        "import io\n",
        "import torch.distributed as dist\n",
        "\n",
        "# process librispeech data\n",
        "\n",
        "\n",
        "# all LibriSpeech files, we download only from 'filter'   \n",
        "LIBRI_SPEECH_URLS = {\n",
        "    \"train\": [\"http://www.openslr.org/resources/12/train-clean-100.tar.gz\",\n",
        "              \"http://www.openslr.org/resources/12/train-clean-360.tar.gz\",\n",
        "              \"http://www.openslr.org/resources/12/train-other-500.tar.gz\"],\n",
        "\n",
        "    \"val\": [\"http://www.openslr.org/resources/12/dev-clean.tar.gz\",\n",
        "            \"http://www.openslr.org/resources/12/dev-other.tar.gz\"],\n",
        "\n",
        "    \"test_clean\": [\"http://www.openslr.org/resources/12/test-clean.tar.gz\"],\n",
        "    \"test_other\": [\"http://www.openslr.org/resources/12/test-other.tar.gz\"]\n",
        "}\n",
        "\n",
        "files_to_dl = [\"train-clean-100.tar.gz\",\"dev-clean.tar.gz\",\"test-clean.tar.gz\"]\n",
        "min_duration, max_duration = 0,15 # \n",
        "target_dl_dir = \"LibriSpeech_dataset/\"\n",
        "sample_rate = 16000\n",
        "\n",
        "def create_manifest(data_path, output_path, min_duration=None, max_duration=None):\n",
        "    file_paths = [os.path.join(dirpath, f)\n",
        "                  for dirpath, dirnames, files in os.walk(data_path)\n",
        "                  for f in fnmatch.filter(files, '*.wav')]\n",
        "    file_paths = order_and_prune_files(file_paths, min_duration, max_duration)\n",
        "    with io.FileIO(output_path, \"w\") as file:\n",
        "        for wav_path in tqdm(file_paths, total=len(file_paths)):\n",
        "            transcript_path = wav_path.replace('/wav/', '/txt/').replace('.wav', '.txt')\n",
        "            sample = os.path.abspath(wav_path) + ',' + os.path.abspath(transcript_path) + '\\n'\n",
        "            file.write(sample.encode('utf-8'))\n",
        "    print('\\n')\n",
        "\n",
        "\n",
        "def _preprocess_transcript(phrase):\n",
        "    return phrase.strip().upper()\n",
        "\n",
        "\n",
        "def _process_file(wav_dir, txt_dir, base_filename, root_dir):\n",
        "    full_recording_path = os.path.join(root_dir, base_filename)\n",
        "    assert os.path.exists(full_recording_path) and os.path.exists(root_dir)\n",
        "    wav_recording_path = os.path.join(wav_dir, base_filename.replace(\".flac\", \".wav\"))\n",
        "    subprocess.call([\"sox {}  -r {} -b 16 -c 1 {}\".format(full_recording_path, str(sample_rate),\n",
        "                                                          wav_recording_path)], shell=True)\n",
        "    # process transcript\n",
        "    txt_transcript_path = os.path.join(txt_dir, base_filename.replace(\".flac\", \".txt\"))\n",
        "    transcript_file = os.path.join(root_dir, \"-\".join(base_filename.split('-')[:-1]) + \".trans.txt\")\n",
        "    assert os.path.exists(transcript_file), \"Transcript file {} does not exist.\".format(transcript_file)\n",
        "    transcriptions = open(transcript_file).read().strip().split(\"\\n\")\n",
        "    transcriptions = {t.split()[0].split(\"-\")[-1]: \" \".join(t.split()[1:]) for t in transcriptions}\n",
        "    with open(txt_transcript_path, \"w\") as f:\n",
        "        key = base_filename.replace(\".flac\", \"\").split(\"-\")[-1]\n",
        "        assert key in transcriptions, \"{} is not in the transcriptions\".format(key)\n",
        "        f.write(_preprocess_transcript(transcriptions[key]))\n",
        "        f.flush()\n",
        "\n",
        "\n",
        "def order_and_prune_files(file_paths, min_duration, max_duration):\n",
        "    print(\"Sorting manifests...\")\n",
        "    duration_file_paths = [(path, float(subprocess.check_output(\n",
        "        ['soxi -D \\\"%s\\\"' % path.strip()], shell=True))) for path in file_paths]\n",
        "    if min_duration and max_duration:\n",
        "        print(\"Pruning manifests between %d and %d seconds\" % (min_duration, max_duration))\n",
        "        duration_file_paths = [(path, duration) for path, duration in duration_file_paths if\n",
        "                               min_duration <= duration <= max_duration]\n",
        "\n",
        "    def func(element):\n",
        "        return element[1]\n",
        "\n",
        "    duration_file_paths.sort(key=func)\n",
        "    return [x[0] for x in duration_file_paths]  # Remove durations\n",
        "\n",
        "\n",
        "def run_download_process():\n",
        "    if not os.path.exists(target_dl_dir):\n",
        "        os.makedirs(target_dl_dir)\n",
        "    for split_type, lst_libri_urls in LIBRI_SPEECH_URLS.items():\n",
        "        split_dir = os.path.join(target_dl_dir, split_type)\n",
        "        if not os.path.exists(split_dir):\n",
        "            os.makedirs(split_dir)\n",
        "        split_wav_dir = os.path.join(split_dir, \"wav\")\n",
        "        if not os.path.exists(split_wav_dir):\n",
        "            os.makedirs(split_wav_dir)\n",
        "        split_txt_dir = os.path.join(split_dir, \"txt\")\n",
        "        if not os.path.exists(split_txt_dir):\n",
        "            os.makedirs(split_txt_dir)\n",
        "        extracted_dir = os.path.join(split_dir, \"LibriSpeech\")\n",
        "        if os.path.exists(extracted_dir):\n",
        "            shutil.rmtree(extracted_dir)\n",
        "        for url in lst_libri_urls:\n",
        "            # check if we want to dl this file\n",
        "            dl_flag = False\n",
        "            for f in files_to_dl:\n",
        "                if url.find(f) != -1:\n",
        "                    dl_flag = True\n",
        "            if not dl_flag:\n",
        "                print(\"Skipping url: {}\".format(url))\n",
        "                continue\n",
        "            filename = url.split(\"/\")[-1]\n",
        "            target_filename = os.path.join(split_dir, filename)\n",
        "            if not os.path.exists(target_filename):\n",
        "                wget.download(url, split_dir)\n",
        "            print(\"Unpacking {}...\".format(filename))\n",
        "            tar = tarfile.open(target_filename)\n",
        "            tar.extractall(split_dir)\n",
        "            tar.close()\n",
        "            os.remove(target_filename)\n",
        "            print(\"Converting flac files to wav and extracting transcripts...\")\n",
        "            assert os.path.exists(extracted_dir), \"Archive {} was not properly uncompressed.\".format(filename)\n",
        "            for root, subdirs, files in tqdm(os.walk(extracted_dir)):\n",
        "                for f in files:\n",
        "                    if f.find(\".flac\") != -1:\n",
        "                        _process_file(wav_dir=split_wav_dir, txt_dir=split_txt_dir,\n",
        "                                      base_filename=f, root_dir=root)\n",
        "\n",
        "            print(\"Finished {}\".format(url))\n",
        "            shutil.rmtree(extracted_dir)\n",
        "        if split_type == 'train':  # Prune to min/max duration\n",
        "            create_manifest(split_dir, 'libri_' + split_type + '_manifest.csv', min_duration, max_duration)\n",
        "        else:\n",
        "            create_manifest(split_dir, 'libri_' + split_type + '_manifest.csv')\n",
        "\n",
        "run_download_process()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Unpacking train-clean-100.tar.gz...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Converting flac files to wav and extracting transcripts...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "838it [11:37,  1.15s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished http://www.openslr.org/resources/12/train-clean-100.tar.gz\n",
            "Skipping url: http://www.openslr.org/resources/12/train-clean-360.tar.gz\n",
            "Skipping url: http://www.openslr.org/resources/12/train-other-500.tar.gz\n",
            "Sorting manifests...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 28539/28539 [00:00<00:00, 52850.80it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Unpacking dev-clean.tar.gz...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Converting flac files to wav and extracting transcripts...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "139it [00:49,  3.21it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished http://www.openslr.org/resources/12/dev-clean.tar.gz\n",
            "Skipping url: http://www.openslr.org/resources/12/dev-other.tar.gz\n",
            "Sorting manifests...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2703/2703 [00:00<00:00, 50692.63it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Unpacking test-clean.tar.gz...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Converting flac files to wav and extracting transcripts...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "129it [00:47,  3.59it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished http://www.openslr.org/resources/12/test-clean.tar.gz\n",
            "Sorting manifests...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2620/2620 [00:00<00:00, 49411.98it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Skipping url: http://www.openslr.org/resources/12/test-other.tar.gz\n",
            "Sorting manifests...\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "JSKnJOQrqJqt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ]
    },
    {
      "metadata": {
        "id": "pw_gi9e0Eg0G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -r warp-ctc\n",
        "!git clone https://github.com/SeanNaren/warp-ctc.git\n",
        "!cd warp-ctc; mkdir build; cd build; cmake ..;make\n",
        "!cd warp-ctc/pytorch_binding;python setup.py install\n",
        "# to reimport a module Python 2\n",
        "# reload(pytorch_binding)\n",
        "# or Python 3\n",
        "import importlib\n",
        "importlib.reload(pytorch_binding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h8pmndhlQCbu",
        "colab_type": "code",
        "outputId": "ac04ceba-2806-4eb3-9e77-8d101e086b46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install git+git://github.com/pytorch/audio"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/pytorch/audio\n",
            "  Cloning git://github.com/pytorch/audio to /tmp/pip-req-build-5db7r9a1\n",
            "Requirement already satisfied (use --upgrade to upgrade): torchaudio==0.2 from git+git://github.com/pytorch/audio in /usr/local/lib/python3.6/dist-packages\n",
            "Building wheels for collected packages: torchaudio\n",
            "  Building wheel for torchaudio (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-1d03jg9v/wheels/4e/7c/c5/0d946acbaccad9fe62590374454c4cf135846c9c96fce3ac75\n",
            "Successfully built torchaudio\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pw6Rjt0QOIbl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "\n",
        "windows = {'hamming': scipy.signal.hamming, 'hann': scipy.signal.hann, 'blackman': scipy.signal.blackman,\n",
        "           'bartlett': scipy.signal.bartlett}\n",
        "\n",
        "def load_audio(path):\n",
        "    sound, _ = torchaudio.load(path, normalization=True)\n",
        "    sound = sound.numpy().T\n",
        "    if len(sound.shape) > 1:\n",
        "        if sound.shape[1] == 1:\n",
        "            sound = sound.squeeze()\n",
        "        else:\n",
        "            sound = sound.mean(axis=1)  # multiple channels, average\n",
        "    return sound\n",
        "  \n",
        "class AudioParser(object):\n",
        "    def parse_transcript(self, transcript_path):\n",
        "        \"\"\"\n",
        "        :param transcript_path: Path where transcript is stored from the manifest file\n",
        "        :return: Transcript in training/testing format\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def parse_audio(self, audio_path):\n",
        "        \"\"\"\n",
        "        :param audio_path: Path where audio is stored from the manifest file\n",
        "        :return: Audio in training/testing format\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class NoiseInjection(object):\n",
        "    def __init__(self,\n",
        "                 path=None,\n",
        "                 sample_rate=16000,\n",
        "                 noise_levels=(0, 0.5)):\n",
        "        \"\"\"\n",
        "        Adds noise to an input signal with specific SNR. Higher the noise level, the more noise added.\n",
        "        Modified code from https://github.com/willfrey/audio/blob/master/torchaudio/transforms.py\n",
        "        \"\"\"\n",
        "        if not os.path.exists(path):\n",
        "            print(\"Directory doesn't exist: {}\".format(path))\n",
        "            raise IOError\n",
        "        self.paths = path is not None and librosa.util.find_files(path)\n",
        "        self.sample_rate = sample_rate\n",
        "        self.noise_levels = noise_levels\n",
        "\n",
        "    def inject_noise(self, data):\n",
        "        noise_path = np.random.choice(self.paths)\n",
        "        noise_level = np.random.uniform(*self.noise_levels)\n",
        "        return self.inject_noise_sample(data, noise_path, noise_level)\n",
        "\n",
        "    def inject_noise_sample(self, data, noise_path, noise_level):\n",
        "        noise_len = get_audio_length(noise_path)\n",
        "        data_len = len(data) / self.sample_rate\n",
        "        noise_start = np.random.rand() * (noise_len - data_len)\n",
        "        noise_end = noise_start + data_len\n",
        "        noise_dst = audio_with_sox(noise_path, self.sample_rate, noise_start, noise_end)\n",
        "        assert len(data) == len(noise_dst)\n",
        "        noise_energy = np.sqrt(noise_dst.dot(noise_dst) / noise_dst.size)\n",
        "        data_energy = np.sqrt(data.dot(data) / data.size)\n",
        "        data += noise_level * noise_dst * data_energy / noise_energy\n",
        "        return data\n",
        "\n",
        "\n",
        "class SpectrogramParser(AudioParser):\n",
        "    def __init__(self, audio_conf, normalize=False, augment=False):\n",
        "        \"\"\"\n",
        "        Parses audio file into spectrogram with optional normalization and various augmentations\n",
        "        :param audio_conf: Dictionary containing the sample rate, window and the window length/stride in seconds\n",
        "        :param normalize(default False):  Apply standard mean and deviation normalization to audio tensor\n",
        "        :param augment(default False):  Apply random tempo and gain perturbations\n",
        "        \"\"\"\n",
        "        super(SpectrogramParser, self).__init__()\n",
        "        self.window_stride = audio_conf['window_stride']\n",
        "        self.window_size = audio_conf['window_size']\n",
        "        self.sample_rate = audio_conf['sample_rate']\n",
        "        self.window = windows.get(audio_conf['window'], windows['hamming'])\n",
        "        self.normalize = normalize\n",
        "        self.augment = augment\n",
        "        self.noiseInjector = NoiseInjection(audio_conf['noise_dir'], self.sample_rate,\n",
        "                                            audio_conf['noise_levels']) if audio_conf.get(\n",
        "            'noise_dir') is not None else None\n",
        "        self.noise_prob = audio_conf.get('noise_prob')\n",
        "\n",
        "    def parse_audio(self, audio_path):\n",
        "        if self.augment:\n",
        "            y = load_randomly_augmented_audio(audio_path, self.sample_rate)\n",
        "        else:\n",
        "            y = load_audio(audio_path)\n",
        "        if self.noiseInjector:\n",
        "            add_noise = np.random.binomial(1, self.noise_prob)\n",
        "            if add_noise:\n",
        "                y = self.noiseInjector.inject_noise(y)\n",
        "        n_fft = int(self.sample_rate * self.window_size)\n",
        "        win_length = n_fft\n",
        "        hop_length = int(self.sample_rate * self.window_stride)\n",
        "        # STFT\n",
        "        D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,\n",
        "                         win_length=win_length, window=self.window)\n",
        "        spect, phase = librosa.magphase(D)\n",
        "        # S = log(S+1)\n",
        "        spect = np.log1p(spect)\n",
        "        spect = torch.FloatTensor(spect)\n",
        "        if self.normalize:\n",
        "            mean = spect.mean()\n",
        "            std = spect.std()\n",
        "            spect.add_(-mean)\n",
        "            spect.div_(std)\n",
        "\n",
        "        return spect\n",
        "\n",
        "    def parse_transcript(self, transcript_path):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class SpectrogramDataset(Dataset, SpectrogramParser):\n",
        "    def __init__(self, audio_conf, manifest_filepath, labels, normalize=False, augment=False):\n",
        "        \"\"\"\n",
        "        Dataset that loads tensors via a csv containing file paths to audio files and transcripts separated by\n",
        "        a comma. Each new line is a different sample. Example below:\n",
        "        /path/to/audio.wav,/path/to/audio.txt\n",
        "        ...\n",
        "        :param audio_conf: Dictionary containing the sample rate, window and the window length/stride in seconds\n",
        "        :param manifest_filepath: Path to manifest csv as describe above\n",
        "        :param labels: String containing all the possible characters to map to\n",
        "        :param normalize: Apply standard mean and deviation normalization to audio tensor\n",
        "        :param augment(default False):  Apply random tempo and gain perturbations\n",
        "        \"\"\"\n",
        "        with open(manifest_filepath) as f:\n",
        "            ids = f.readlines()\n",
        "        ids = [x.strip().split(',') for x in ids]\n",
        "        self.ids = ids\n",
        "        self.size = len(ids)\n",
        "        self.labels_map = dict([(labels[i], i) for i in range(len(labels))])\n",
        "        super(SpectrogramDataset, self).__init__(audio_conf, normalize, augment)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.ids[index]\n",
        "        audio_path, transcript_path = sample[0], sample[1]\n",
        "        spect = self.parse_audio(audio_path)\n",
        "        transcript = self.parse_transcript(transcript_path)\n",
        "        return spect, transcript\n",
        "\n",
        "    def parse_transcript(self, transcript_path):\n",
        "        with open(transcript_path, 'r', encoding='utf8') as transcript_file:\n",
        "            transcript = transcript_file.read().replace('\\n', '')\n",
        "        transcript = list(filter(None, [self.labels_map.get(x) for x in list(transcript)]))\n",
        "        return transcript\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "\n",
        "def _collate_fn(batch):\n",
        "    def func(p):\n",
        "        return p[0].size(1)\n",
        "\n",
        "    batch = sorted(batch, key=lambda sample: sample[0].size(1), reverse=True)\n",
        "    longest_sample = max(batch, key=func)[0]\n",
        "    freq_size = longest_sample.size(0)\n",
        "    minibatch_size = len(batch)\n",
        "    max_seqlength = longest_sample.size(1)\n",
        "    inputs = torch.zeros(minibatch_size, 1, freq_size, max_seqlength)\n",
        "    input_percentages = torch.FloatTensor(minibatch_size)\n",
        "    target_sizes = torch.IntTensor(minibatch_size)\n",
        "    targets = []\n",
        "    for x in range(minibatch_size):\n",
        "        sample = batch[x]\n",
        "        tensor = sample[0]\n",
        "        target = sample[1]\n",
        "        seq_length = tensor.size(1)\n",
        "        inputs[x][0].narrow(1, 0, seq_length).copy_(tensor)\n",
        "        input_percentages[x] = seq_length / float(max_seqlength)\n",
        "        target_sizes[x] = len(target)\n",
        "        targets.extend(target)\n",
        "    targets = torch.IntTensor(targets)\n",
        "    return inputs, targets, input_percentages, target_sizes\n",
        "\n",
        "\n",
        "class AudioDataLoader(DataLoader):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Creates a data loader for AudioDatasets.\n",
        "        \"\"\"\n",
        "        super(AudioDataLoader, self).__init__(*args, **kwargs)\n",
        "        self.collate_fn = _collate_fn\n",
        "\n",
        "\n",
        "class BucketingSampler(Sampler):\n",
        "    def __init__(self, data_source, batch_size=1):\n",
        "        \"\"\"\n",
        "        Samples batches assuming they are in order of size to batch similarly sized samples together.\n",
        "        \"\"\"\n",
        "        super(BucketingSampler, self).__init__(data_source)\n",
        "        self.data_source = data_source\n",
        "        ids = list(range(0, len(data_source)))\n",
        "        self.bins = [ids[i:i + batch_size] for i in range(0, len(ids), batch_size)]\n",
        "\n",
        "    def __iter__(self):\n",
        "        for ids in self.bins:\n",
        "            np.random.shuffle(ids)\n",
        "            yield ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.bins)\n",
        "\n",
        "    def shuffle(self, epoch):\n",
        "        np.random.shuffle(self.bins)\n",
        "\n",
        "\n",
        "class DistributedBucketingSampler(Sampler):\n",
        "    def __init__(self, data_source, batch_size=1, num_replicas=None, rank=None):\n",
        "        \"\"\"\n",
        "        Samples batches assuming they are in order of size to batch similarly sized samples together.\n",
        "        \"\"\"\n",
        "        super(DistributedBucketingSampler, self).__init__(data_source)\n",
        "        if num_replicas is None:\n",
        "            num_replicas = get_world_size()\n",
        "        if rank is None:\n",
        "            rank = get_rank()\n",
        "        self.data_source = data_source\n",
        "        self.ids = list(range(0, len(data_source)))\n",
        "        self.batch_size = batch_size\n",
        "        self.bins = [self.ids[i:i + batch_size] for i in range(0, len(self.ids), batch_size)]\n",
        "        self.num_replicas = num_replicas\n",
        "        self.rank = rank\n",
        "        self.num_samples = int(math.ceil(len(self.bins) * 1.0 / self.num_replicas))\n",
        "        self.total_size = self.num_samples * self.num_replicas\n",
        "\n",
        "    def __iter__(self):\n",
        "        offset = self.rank\n",
        "        # add extra samples to make it evenly divisible\n",
        "        bins = self.bins + self.bins[:(self.total_size - len(self.bins))]\n",
        "        assert len(bins) == self.total_size\n",
        "        samples = bins[offset::self.num_replicas]  # Get every Nth bin, starting from rank\n",
        "        return iter(samples)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def shuffle(self, epoch):\n",
        "        # deterministically shuffle based on epoch\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(epoch)\n",
        "        bin_ids = list(torch.randperm(len(self.bins), generator=g))\n",
        "        self.bins = [self.bins[i] for i in bin_ids]\n",
        "\n",
        "\n",
        "def get_audio_length(path):\n",
        "    output = subprocess.check_output(['soxi -D \\\"%s\\\"' % path.strip()], shell=True)\n",
        "    return float(output)\n",
        "\n",
        "\n",
        "def audio_with_sox(path, sample_rate, start_time, end_time):\n",
        "    \"\"\"\n",
        "    crop and resample the recording with sox and loads it.\n",
        "    \"\"\"\n",
        "    with NamedTemporaryFile(suffix=\".wav\") as tar_file:\n",
        "        tar_filename = tar_file.name\n",
        "        sox_params = \"sox \\\"{}\\\" -r {} -c 1 -b 16 -e si {} trim {} ={} >/dev/null 2>&1\".format(path, sample_rate,\n",
        "                                                                                               tar_filename, start_time,\n",
        "                                                                                               end_time)\n",
        "        os.system(sox_params)\n",
        "        y = load_audio(tar_filename)\n",
        "        return y\n",
        "\n",
        "\n",
        "def augment_audio_with_sox(path, sample_rate, tempo, gain):\n",
        "    \"\"\"\n",
        "    Changes tempo and gain of the recording with sox and loads it.\n",
        "    \"\"\"\n",
        "    with NamedTemporaryFile(suffix=\".wav\") as augmented_file:\n",
        "        augmented_filename = augmented_file.name\n",
        "        sox_augment_params = [\"tempo\", \"{:.3f}\".format(tempo), \"gain\", \"{:.3f}\".format(gain)]\n",
        "        sox_params = \"sox \\\"{}\\\" -r {} -c 1 -b 16 -e si {} {} >/dev/null 2>&1\".format(path, sample_rate,\n",
        "                                                                                      augmented_filename,\n",
        "                                                                                      \" \".join(sox_augment_params))\n",
        "        os.system(sox_params)\n",
        "        y = load_audio(augmented_filename)\n",
        "        return y\n",
        "\n",
        "\n",
        "def load_randomly_augmented_audio(path, sample_rate=16000, tempo_range=(0.85, 1.15),\n",
        "                                  gain_range=(-6, 8)):\n",
        "    \"\"\"\n",
        "    Picks tempo and gain uniformly, applies it to the utterance by using sox utility.\n",
        "    Returns the augmented utterance.\n",
        "    \"\"\"\n",
        "    low_tempo, high_tempo = tempo_range\n",
        "    tempo_value = np.random.uniform(low=low_tempo, high=high_tempo)\n",
        "    low_gain, high_gain = gain_range\n",
        "    gain_value = np.random.uniform(low=low_gain, high=high_gain)\n",
        "    audio = augment_audio_with_sox(path=path, sample_rate=sample_rate,\n",
        "                                   tempo=tempo_value, gain=gain_value)\n",
        "    return audio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AfPEYbQT84LD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 56\n",
        "num_workers = 1 # Number of workers used in data-loading\n",
        "\n",
        "\n",
        "# prepare data_sets and data_loaders\n",
        "audio_conf = dict(sample_rate=16000,\n",
        "                window_size=.02, # Window size for spectrogram in seconds\n",
        "                window_stride=.01, # Window stride for spectrogram in seconds\n",
        "                window='hamming', # Window type for spectrogram generation\n",
        "                noise_dir=None, # Directory to inject noise into audio. If default, noise Inject not added\n",
        "                noise_prob=0.4, # Probability of noise being added per sample\n",
        "                noise_levels=(0.0, 0.5)) # Minimum noise level to sample from. (1.0 means all noise, not original signal\n",
        "\n",
        "with open('labels.json') as label_file:\n",
        "  labels = str(''.join(json.load(label_file)))\n",
        "\n",
        "train_dataset = SpectrogramDataset(audio_conf=audio_conf, manifest_filepath='libri_train_manifest.csv', labels=labels,\n",
        "                                       normalize=True, augment=False)\n",
        "test_dataset = SpectrogramDataset(audio_conf=audio_conf, manifest_filepath='libri_val_manifest.csv', labels=labels,\n",
        "                                  normalize=True, augment=False)\n",
        "\n",
        "train_sampler = BucketingSampler(train_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "train_loader = AudioDataLoader(train_dataset,\n",
        "                               num_workers=num_workers, batch_sampler=train_sampler)\n",
        "test_loader = AudioDataLoader(test_dataset, batch_size=batch_size,\n",
        "                              num_workers=num_workers)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ejYc0GSD89uL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "from warpctc_pytorch import CTCLoss\n",
        "\n",
        "# define model & train\n",
        "supported_rnns = {\n",
        "    'lstm': nn.LSTM,\n",
        "    'rnn': nn.RNN,\n",
        "    'gru': nn.GRU\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_6xkGoxtM4Ed",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        \n",
        "def check_loss(loss, loss_value):\n",
        "    \"\"\"\n",
        "    Check that warp-ctc loss is valid and will not break training\n",
        "    :return: Return if loss is valid, and the error in case it is not\n",
        "    \"\"\"\n",
        "    loss_valid = True\n",
        "    error = ''\n",
        "    if loss_value == float(\"inf\") or loss_value == float(\"-inf\"):\n",
        "        loss_valid = False\n",
        "        error = \"WARNING: received an inf loss\"\n",
        "    elif torch.isnan(loss).sum() > 0:\n",
        "        loss_valid = False\n",
        "        error = 'WARNING: received a nan loss, setting loss value to 0'\n",
        "    elif loss_value < 0:\n",
        "        loss_valid = False\n",
        "        error = \"WARNING: received a negative loss\"\n",
        "    return loss_valid, error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZJRkgUQeGE6W",
        "colab_type": "code",
        "outputId": "42da3bd8-ece0-416a-ffc9-5559dccb9962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        }
      },
      "cell_type": "code",
      "source": [
        "# train\n",
        "import time\n",
        "\n",
        "rnn_hidden_size=800\n",
        "lr = 3e-4\n",
        "momentum = 0.9\n",
        "start_epoch = 0\n",
        "epochs = 70\n",
        "max_norm = 400\n",
        "silent = False\n",
        "checkpoint = 'checkpoint'\n",
        "checkpoint_per_batch = 0 \n",
        "mixed_precision = 'store_true'\n",
        "learning_anneal = 1.1\n",
        "model_path = 'models/deepspeech_final.pth'\n",
        "no_shuffle = False\n",
        "save_folder = './'\n",
        "\n",
        "model = DeepSpeech(rnn_hidden_size=rnn_hidden_size,\n",
        "                   nb_layers=4, # Number of RNN layers\n",
        "                   labels=labels,\n",
        "                   rnn_type=supported_rnns['gru'],\n",
        "                   audio_conf=audio_conf,\n",
        "                   bidirectional=True) # use BiLSTM by default\n",
        "\n",
        "parameters = model.parameters()\n",
        "\n",
        "criterion = CTCLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(parameters, lr=lr,\n",
        "                                momentum=momentum, nesterov=True, weight_decay=1e-5)\n",
        "batch_time = AverageMeter()\n",
        "data_time = AverageMeter()\n",
        "losses = AverageMeter()\n",
        "\n",
        "avg_loss, start_epoch, start_iter, optim_state = 0, 0, 0, None\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "        model.train()\n",
        "        end = time.time()\n",
        "        start_epoch_time = time.time()\n",
        "        for i, (data) in enumerate(train_loader, start=start_iter):\n",
        "            if i == len(train_sampler):\n",
        "                break\n",
        "            inputs, targets, input_percentages, target_sizes = data\n",
        "            input_sizes = input_percentages.mul_(int(inputs.size(3))).int()\n",
        "            # measure data loading time\n",
        "            data_time.update(time.time() - end)\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            out, output_sizes = model(inputs, input_sizes)\n",
        "            out = out.transpose(0, 1)  # TxNxH\n",
        "\n",
        "            float_out = out.float()  # ensure float32 for loss\n",
        "            loss = criterion(float_out, targets, output_sizes, target_sizes).to(device)\n",
        "            loss = loss / inputs.size(0)  # average the loss by minibatch\n",
        "\n",
        "#             if args.distributed:\n",
        "#                 loss = loss.to(device)\n",
        "#                 loss_value = reduce_tensor(loss, args.world_size).item()\n",
        "#             else:\n",
        "            loss_value = loss.item()\n",
        "\n",
        "            # Check to ensure valid loss was calculated\n",
        "            valid_loss, error = check_loss(loss, loss_value)\n",
        "            if valid_loss:\n",
        "                optimizer.zero_grad()\n",
        "                # compute gradient\n",
        "#                 if args.mixed_precision:\n",
        "#                     optimizer.backward(loss)\n",
        "#                     optimizer.clip_master_grads(args.max_norm)\n",
        "#                 else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "                optimizer.step()\n",
        "            else:\n",
        "                print(error)\n",
        "                print('Skipping grad update')\n",
        "                loss_value = 0\n",
        "\n",
        "            avg_loss += loss_value\n",
        "            losses.update(loss_value, inputs.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "            if not silent:\n",
        "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                      'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
        "                    (epoch + 1), (i + 1), len(train_sampler), batch_time=batch_time, data_time=data_time, loss=losses))\n",
        "            if checkpoint_per_batch > 0 and i > 0 and (i + 1) % checkpoint_per_batch == 0 and main_proc:\n",
        "                file_path = '%s/deepspeech_checkpoint_epoch_%d_iter_%d.pth' % (save_folder, epoch + 1, i + 1)\n",
        "                print(\"Saving checkpoint model to %s\" % file_path)\n",
        "                torch.save(DeepSpeech.serialize(model, optimizer=optimizer, epoch=epoch, iteration=i,\n",
        "                                                loss_results=loss_results,\n",
        "                                                wer_results=wer_results, cer_results=cer_results, avg_loss=avg_loss),\n",
        "                           file_path)\n",
        "            del loss, out, float_out\n",
        "\n",
        "        avg_loss /= len(train_sampler)\n",
        "\n",
        "        epoch_time = time.time() - start_epoch_time\n",
        "        print('Training Summary Epoch: [{0}]\\t'\n",
        "              'Time taken (s): {epoch_time:.0f}\\t'\n",
        "              'Average Loss {loss:.3f}\\t'.format(epoch + 1, epoch_time=epoch_time, loss=avg_loss))\n",
        "\n",
        "        start_iter = 0  # Reset start iteration for next epoch\n",
        "        total_cer, total_wer = 0, 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (data) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
        "                inputs, targets, input_percentages, target_sizes = data\n",
        "                input_sizes = input_percentages.mul_(int(inputs.size(3))).int()\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                # unflatten targets\n",
        "                split_targets = []\n",
        "                offset = 0\n",
        "                for size in target_sizes:\n",
        "                    split_targets.append(targets[offset:offset + size])\n",
        "                    offset += size\n",
        "\n",
        "                out, output_sizes = model(inputs, input_sizes)\n",
        "\n",
        "                decoded_output, _ = decoder.decode(out, output_sizes)\n",
        "                target_strings = decoder.convert_to_strings(split_targets)\n",
        "                wer, cer = 0, 0\n",
        "                for x in range(len(target_strings)):\n",
        "                    transcript, reference = decoded_output[x][0], target_strings[x][0]\n",
        "                    wer += decoder.wer(transcript, reference) / float(len(reference.split()))\n",
        "                    cer += decoder.cer(transcript, reference) / float(len(reference))\n",
        "                total_cer += cer\n",
        "                total_wer += wer\n",
        "                del out\n",
        "            wer = total_wer / len(test_loader.dataset)\n",
        "            cer = total_cer / len(test_loader.dataset)\n",
        "            wer *= 100\n",
        "            cer *= 100\n",
        "            loss_results[epoch] = avg_loss\n",
        "            wer_results[epoch] = wer\n",
        "            cer_results[epoch] = cer\n",
        "            print('Validation Summary Epoch: [{0}]\\t'\n",
        "                  'Average WER {wer:.3f}\\t'\n",
        "                  'Average CER {cer:.3f}\\t'.format(\n",
        "                epoch + 1, wer=wer, cer=cer))\n",
        "\n",
        "        values = {\n",
        "            'loss_results': loss_results,\n",
        "            'cer_results': cer_results,\n",
        "            'wer_results': wer_results\n",
        "        }\n",
        "#         if args.visdom and main_proc:\n",
        "#             visdom_logger.update(epoch, values)\n",
        "#         if args.tensorboard and main_proc:\n",
        "#             tensorboard_logger.update(epoch, values, model.named_parameters())\n",
        "#             values = {\n",
        "#                 'Avg Train Loss': avg_loss,\n",
        "#                 'Avg WER': wer,\n",
        "#                 'Avg CER': cer\n",
        "#             }\n",
        "\n",
        "        if main_proc and checkpoint:\n",
        "            file_path = '%s/deepspeech_%d.pth.tar' % (save_folder, epoch + 1)\n",
        "            torch.save(DeepSpeech.serialize(model, optimizer=optimizer, epoch=epoch, loss_results=loss_results,\n",
        "                                            wer_results=wer_results, cer_results=cer_results),\n",
        "                       file_path)\n",
        "        # anneal lr\n",
        "        param_groups = optimizer.optimizer.param_groups if mixed_precision else optimizer.param_groups\n",
        "        for g in param_groups:\n",
        "            g['lr'] = g['lr'] / learning_anneal\n",
        "        print('Learning rate annealed to: {lr:.6f}'.format(lr=g['lr']))\n",
        "\n",
        "        if main_proc and (best_wer is None or best_wer > wer):\n",
        "            print(\"Found better validated model, saving to %s\" % model_path)\n",
        "            torch.save(DeepSpeech.serialize(model, optimizer=optimizer, epoch=epoch, loss_results=loss_results,\n",
        "                                            wer_results=wer_results, cer_results=cer_results)\n",
        "                       , model_path)\n",
        "            best_wer = wer\n",
        "\n",
        "            avg_loss = 0\n",
        "            if not no_shuffle:\n",
        "                print(\"Shuffling batches...\")\n",
        "                train_sampler.shuffle(epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [1][157/510]\tTime 9.319 (6.043)\tData 0.012 (0.011)\tLoss 352.9880 (306.6018)\t\n",
            "Epoch: [1][158/510]\tTime 9.226 (6.063)\tData 0.011 (0.011)\tLoss 376.5805 (307.0447)\t\n",
            "Epoch: [1][159/510]\tTime 9.206 (6.083)\tData 0.012 (0.011)\tLoss 370.1011 (307.4413)\t\n",
            "Epoch: [1][160/510]\tTime 9.233 (6.103)\tData 0.012 (0.011)\tLoss 371.3450 (307.8407)\t\n",
            "Epoch: [1][161/510]\tTime 9.231 (6.122)\tData 0.012 (0.011)\tLoss 367.8272 (308.2133)\t\n",
            "Epoch: [1][162/510]\tTime 9.256 (6.142)\tData 0.011 (0.011)\tLoss 365.0407 (308.5641)\t\n",
            "Epoch: [1][163/510]\tTime 9.282 (6.161)\tData 0.012 (0.011)\tLoss 373.0891 (308.9599)\t\n",
            "Epoch: [1][164/510]\tTime 9.315 (6.180)\tData 0.011 (0.011)\tLoss 349.5284 (309.2073)\t\n",
            "Epoch: [1][165/510]\tTime 9.297 (6.199)\tData 0.012 (0.011)\tLoss 356.9648 (309.4967)\t\n",
            "Epoch: [1][166/510]\tTime 9.314 (6.218)\tData 0.012 (0.011)\tLoss 351.9535 (309.7525)\t\n",
            "Epoch: [1][167/510]\tTime 9.283 (6.236)\tData 0.011 (0.011)\tLoss 361.0645 (310.0598)\t\n",
            "Epoch: [1][168/510]\tTime 9.367 (6.255)\tData 0.012 (0.011)\tLoss 342.3700 (310.2521)\t\n",
            "Epoch: [1][169/510]\tTime 9.386 (6.273)\tData 0.012 (0.011)\tLoss 341.3559 (310.4361)\t\n",
            "Epoch: [1][170/510]\tTime 9.451 (6.292)\tData 0.011 (0.011)\tLoss 346.8792 (310.6505)\t\n",
            "Epoch: [1][171/510]\tTime 9.351 (6.310)\tData 0.011 (0.011)\tLoss 360.2163 (310.9404)\t\n",
            "Epoch: [1][172/510]\tTime 9.558 (6.329)\tData 0.012 (0.011)\tLoss 344.1071 (311.1332)\t\n",
            "Epoch: [1][173/510]\tTime 9.395 (6.346)\tData 0.012 (0.011)\tLoss 349.7697 (311.3565)\t\n",
            "Epoch: [1][174/510]\tTime 9.448 (6.364)\tData 0.011 (0.011)\tLoss 347.8656 (311.5663)\t\n",
            "Epoch: [1][175/510]\tTime 9.460 (6.382)\tData 0.012 (0.011)\tLoss 343.8571 (311.7509)\t\n",
            "Epoch: [1][176/510]\tTime 9.485 (6.400)\tData 0.011 (0.011)\tLoss 352.3412 (311.9815)\t\n",
            "Epoch: [1][177/510]\tTime 9.541 (6.417)\tData 0.011 (0.011)\tLoss 358.9838 (312.2470)\t\n",
            "Epoch: [1][178/510]\tTime 9.443 (6.434)\tData 0.012 (0.011)\tLoss 355.9801 (312.4927)\t\n",
            "Epoch: [1][179/510]\tTime 9.500 (6.451)\tData 0.012 (0.011)\tLoss 348.8768 (312.6960)\t\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-83f9557f87dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m#                     optimizer.clip_master_grads(args.max_norm)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m#                 else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 487.38 MiB (GPU 0; 11.17 GiB total capacity; 9.44 GiB already allocated; 278.56 MiB free; 1.13 GiB cached)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "NDBp1r5_TKac",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# del optimizer, model, parameters, criterion\n",
        "# del data\n",
        "# del float_out\n",
        "torch.cuda.empty_cache()\n",
        "# torch.cuda.is_available()\n",
        "# torch.cuda.memory_allocated()\n",
        "# torch.cuda.memory_cached()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gEE30UxtVeot",
        "colab_type": "code",
        "outputId": "1ea03e20-28a3-46af-9a59-cc3b0680dc80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.max_memory_allocated()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11289685504"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "JfZejwMhlbOa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!nvidia-smi "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i9ncUPU7na-8",
        "colab_type": "code",
        "outputId": "e2546f03-c1b0-43bf-ed81-9bb96e3da898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install python3-gdbm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python3-gdbm\n",
            "\u001b[31m  Could not find a version that satisfies the requirement python3-gdbm (from versions: )\u001b[0m\n",
            "\u001b[31mNo matching distribution found for python3-gdbm\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "26f17Im4mUIV",
        "colab_type": "code",
        "outputId": "bd24f2e8-cff5-441c-f243-284e510e9757",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2230
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "for obj in gc.get_objects():\n",
        "    try:\n",
        "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
        "            print(type(obj), obj.size())\n",
        "    except: pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.nn.parameter.Parameter'> torch.Size([29, 800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400, 800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400, 800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400, 800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400, 800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400, 800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400, 800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400, 800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400, 800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400, 800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400, 800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400, 800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400, 800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400, 800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400, 1312])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400, 800])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2400, 1312])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([32, 32, 21, 11])\n",
            "<class 'torch.Tensor'> torch.Size([32])\n",
            "<class 'torch.Tensor'> torch.Size([32])\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "<class 'torch.Tensor'> torch.Size([800])\n",
            "<class 'torch.Tensor'> torch.Size([800])\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "<class 'torch.Tensor'> torch.Size([800])\n",
            "<class 'torch.Tensor'> torch.Size([800])\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "<class 'torch.Tensor'> torch.Size([800])\n",
            "<class 'torch.Tensor'> torch.Size([800])\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "<class 'torch.Tensor'> torch.Size([800])\n",
            "<class 'torch.Tensor'> torch.Size([800])\n",
            "<class 'torch.Tensor'> torch.Size([])\n",
            "<class 'torch.Tensor'> torch.Size([10402])\n",
            "<class 'torch.Tensor'> torch.Size([56])\n",
            "<class 'torch.Tensor'> torch.Size([56])\n",
            "<class 'torch.Tensor'> torch.Size([56])\n",
            "<class 'torch.Tensor'> torch.Size([56, 1, 161, 1296])\n",
            "<class 'torch.Tensor'> torch.Size([56])\n",
            "<class 'torch.Tensor'> torch.Size([648, 56, 29])\n",
            "<class 'torch.Tensor'> torch.Size([1])\n",
            "<class 'torch.Tensor'> torch.Size([32, 1, 41, 11])\n",
            "<class 'torch.Tensor'> torch.Size([32])\n",
            "<class 'torch.Tensor'> torch.Size([32])\n",
            "<class 'torch.Tensor'> torch.Size([32])\n",
            "<class 'torch.Tensor'> torch.Size([32, 32, 21, 11])\n",
            "<class 'torch.Tensor'> torch.Size([32])\n",
            "<class 'torch.Tensor'> torch.Size([32])\n",
            "<class 'torch.Tensor'> torch.Size([32])\n",
            "<class 'torch.Tensor'> torch.Size([2400, 1312])\n",
            "<class 'torch.Tensor'> torch.Size([2400, 800])\n",
            "<class 'torch.Tensor'> torch.Size([2400])\n",
            "<class 'torch.Tensor'> torch.Size([2400])\n",
            "<class 'torch.Tensor'> torch.Size([2400, 1312])\n",
            "<class 'torch.Tensor'> torch.Size([2400, 800])\n",
            "<class 'torch.Tensor'> torch.Size([2400])\n",
            "<class 'torch.Tensor'> torch.Size([2400])\n",
            "<class 'torch.Tensor'> torch.Size([800])\n",
            "<class 'torch.Tensor'> torch.Size([800])\n",
            "<class 'torch.Tensor'> torch.Size([2400, 800])\n",
            "<class 'torch.Tensor'> torch.Size([2400, 800])\n",
            "<class 'torch.Tensor'> torch.Size([2400])\n",
            "<class 'torch.Tensor'> torch.Size([2400])\n",
            "<class 'torch.Tensor'> torch.Size([2400, 800])\n",
            "<class 'torch.Tensor'> torch.Size([2400, 800])\n",
            "<class 'torch.Tensor'> torch.Size([2400])\n",
            "<class 'torch.Tensor'> torch.Size([2400])\n",
            "<class 'torch.Tensor'> torch.Size([800])\n",
            "<class 'torch.Tensor'> torch.Size([800])\n",
            "<class 'torch.Tensor'> torch.Size([2400, 800])\n",
            "<class 'torch.Tensor'> torch.Size([2400, 800])\n",
            "<class 'torch.Tensor'> torch.Size([2400])\n",
            "<class 'torch.Tensor'> torch.Size([2400])\n",
            "<class 'torch.Tensor'> torch.Size([2400, 800])\n",
            "<class 'torch.Tensor'> torch.Size([2400, 800])\n",
            "<class 'torch.Tensor'> torch.Size([2400])\n",
            "<class 'torch.Tensor'> torch.Size([2400])\n",
            "<class 'torch.Tensor'> torch.Size([800])\n",
            "<class 'torch.Tensor'> torch.Size([800])\n",
            "<class 'torch.Tensor'> torch.Size([2400, 800])\n",
            "<class 'torch.Tensor'> torch.Size([2400, 800])\n",
            "<class 'torch.Tensor'> torch.Size([2400])\n",
            "<class 'torch.Tensor'> torch.Size([2400])\n",
            "<class 'torch.Tensor'> torch.Size([2400, 800])\n",
            "<class 'torch.Tensor'> torch.Size([2400, 800])\n",
            "<class 'torch.Tensor'> torch.Size([2400])\n",
            "<class 'torch.Tensor'> torch.Size([2400])\n",
            "<class 'torch.Tensor'> torch.Size([800])\n",
            "<class 'torch.Tensor'> torch.Size([800])\n",
            "<class 'torch.Tensor'> torch.Size([29, 800])\n",
            "<class 'torch.Tensor'> torch.Size([56, 1, 161, 1296])\n",
            "<class 'torch.Tensor'> torch.Size([648, 56, 29])\n",
            "<class 'torch.Tensor'> torch.Size([1])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([32, 1, 41, 11])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
            "<class 'torch.Tensor'> torch.Size([32])\n",
            "<class 'torch.Tensor'> torch.Size([32])\n",
            "<class 'torch.Tensor'> torch.Size([])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/distributed/distributed_c10d.py:86: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
            "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "_aWfIHzFkAyg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "locals()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yO2giBHncLVv",
        "colab_type": "code",
        "outputId": "146f9787-979c-4d2f-b980-013a783ddaf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from tqdm import trange\n",
        "dry_runs = 2\n",
        "num_samples = 1024\n",
        "seconds = 15\n",
        "batch_size = 32\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "input_data = torch.randn(num_samples, 1, 161, seconds * 100)\n",
        "input_data = input_data.to(device)\n",
        "input_data = torch.chunk(input_data, int(len(input_data) / batch_size))\n",
        "\n",
        "def iteration(inputs):\n",
        "    # targets, align half of the audio\n",
        "    targets = torch.ones(int(batch_size * ((seconds * 100) / 2)))\n",
        "    target_sizes = torch.empty(batch_size, dtype=torch.int).fill_(int((seconds * 100) / 2))\n",
        "    input_percentages = torch.ones(batch_size).fill_(1)\n",
        "    input_sizes = input_percentages.mul_(int(inputs.size(3))).int()\n",
        "\n",
        "    out, output_sizes = model(inputs, input_sizes)\n",
        "    out = out.transpose(0, 1)  # TxNxH\n",
        "\n",
        "    float_out = out.float()  # ensure float32 for loss\n",
        "    loss = criterion(float_out, targets, output_sizes, target_sizes)\n",
        "    loss = loss / inputs.size(0)  # average the loss by minibatch\n",
        "    optimizer.zero_grad()\n",
        "    # compute gradient\n",
        "    if args.mixed_precision:\n",
        "        optimizer.backward(loss)\n",
        "        optimizer.clip_master_grads(400)\n",
        "    else:\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 400)\n",
        "    optimizer.step()\n",
        "    del loss, out, float_out\n",
        "    \n",
        "def run_benchmark():\n",
        "    print(\"Running dry runs...\")\n",
        "    for n in trange(dry_runs):\n",
        "        for data in tqdm(input_data, total=len(input_data)):\n",
        "            iteration(data)\n",
        "\n",
        "    print(\"\\n Running measured runs...\")\n",
        "    running_time = 0\n",
        "    for n in trange(args.runs):\n",
        "        start_time = time.time()\n",
        "        for data in tqdm(input_data, total=len(input_data)):\n",
        "            iteration(data)\n",
        "        end_time = time.time()\n",
        "        running_time += (end_time - start_time)\n",
        "\n",
        "    return running_time / float(args.runs)\n",
        "\n",
        "\n",
        "run_time = run_benchmark()\n",
        "\n",
        "print(\"\\n Average run time: %.2fs\" % run_time)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-964563623e0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m161\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseconds\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 943.38 MiB (GPU 0; 11.17 GiB total capacity; 10.39 GiB already allocated; 171.06 MiB free; 298.80 MiB cached)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Nn_A_vQ29AJN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# save or load model (also export to Google Drive)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I-HjZ6Vr9J7u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# evaluation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UthxQb1b9H05",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# inference"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "18oH0EzKAXdL",
        "colab_type": "code",
        "outputId": "bdbffee8-b40a-4ad3-dff9-81bb64784fef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "!du ./LibriSpeech_dataset/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10600\t./LibriSpeech_dataset/test_clean/txt\n",
            "613456\t./LibriSpeech_dataset/test_clean/wav\n",
            "624060\t./LibriSpeech_dataset/test_clean\n",
            "10928\t./LibriSpeech_dataset/val/txt\n",
            "611784\t./LibriSpeech_dataset/val/wav\n",
            "622716\t./LibriSpeech_dataset/val\n",
            "4\t./LibriSpeech_dataset/test_other/txt\n",
            "4\t./LibriSpeech_dataset/test_other/wav\n",
            "12\t./LibriSpeech_dataset/test_other\n",
            "115236\t./LibriSpeech_dataset/train/txt\n",
            "11376272\t./LibriSpeech_dataset/train/wav\n",
            "11491512\t./LibriSpeech_dataset/train\n",
            "12738304\t./LibriSpeech_dataset/\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}