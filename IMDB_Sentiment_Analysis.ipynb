{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB Sentiment Analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolaerosca/colab_notebooks/blob/master/IMDB_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "JHb2Hc-hSLSQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Text sentiment analysis can be seen as a classification task. Here I am trying to determine sentiment analysis of a text. "
      ]
    },
    {
      "metadata": {
        "id": "wW00BrsXR-n-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "8ff17923-9804-44ed-ca3b-fd6d7b756634"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import Parameter\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "!pip install wget\n",
        "import wget\n",
        "import io\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "07o1Sntvk3v5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9148bd86-8ad0-418d-e4ac-c7132ece4058"
      },
      "cell_type": "code",
      "source": [
        "# downloading IMDB data set and creating CSV manifest file \n",
        "data_set_url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "target_filename = data_set_url.split(\"/\")[-1]\n",
        "regex_pattern = '(\\d+)_(\\d+)\\.txt'\n",
        "\n",
        "def create_manifest(data_path, output_path):\n",
        "  with io.FileIO(output_path, \"w\") as file:\n",
        "    file.write(\"path,id,score\\n\".encode('utf-8'))\n",
        "    for dirpath, dirnames, files in os.walk(data_path + \"pos\"):\n",
        "      for file_name in files:\n",
        "        m = re.search('(\\d+)_(\\d+)\\.txt', file_name)\n",
        "        review_id = m.group(1)\n",
        "        score = m.group(2)\n",
        "        sample = data_path + \"pos/\" + file_name + ',' + str(review_id) + ',' + str(score) + '\\n'\n",
        "        file.write(sample.encode('utf-8'))\n",
        "    for dirpath, dirnames, files in os.walk(data_path + \"neg\"):\n",
        "      for file_name in files:\n",
        "        m = re.search('(\\d+)_(\\d+)\\.txt', file_name)\n",
        "        review_id = m.group(1)\n",
        "        score = m.group(2)\n",
        "        sample = data_path + \"neg/\" + file_name + ',' + str(review_id) + ',' + str(score) + '\\n'\n",
        "        file.write(sample.encode('utf-8'))\n",
        "\n",
        "if not os.path.exists(target_filename) and not os.path.exists('aclImdb'):\n",
        "  print(\"Downloading {}...\".format(target_filename))\n",
        "  wget.download(data_set_url)\n",
        "if not os.path.exists('aclImdb'):\n",
        "  print(\"Unpacking {}...\".format(target_filename))\n",
        "  tar = tarfile.open(target_filename)\n",
        "  tar.extractall()\n",
        "  tar.close()\n",
        "  os.remove(target_filename)\n",
        "  \n",
        "create_manifest(\"aclImdb/test/\", 'imdb_test_manifest.csv')\n",
        "create_manifest(\"aclImdb/train/\", 'imdb_train_manifest.csv')\n",
        "# os.remove('imdb_test_manifest.csv')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading aclImdb_v1.tar.gz...\n",
            "Unpacking aclImdb_v1.tar.gz...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Qm8kC1lYpDjf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f5401c7f-9561-4aca-984b-c4c5548806d4"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('imdb_train_manifest.csv')\n",
        "df.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "eYJPfQpvGUwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9f66f84d-16d8-4b36-edca-dca5abb8b61d"
      },
      "cell_type": "code",
      "source": [
        "df.query(\"score <= 5\").head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>id</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12500</th>\n",
              "      <td>aclImdb/train/neg/5808_4.txt</td>\n",
              "      <td>5808</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12501</th>\n",
              "      <td>aclImdb/train/neg/6654_1.txt</td>\n",
              "      <td>6654</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12502</th>\n",
              "      <td>aclImdb/train/neg/5731_1.txt</td>\n",
              "      <td>5731</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12503</th>\n",
              "      <td>aclImdb/train/neg/301_1.txt</td>\n",
              "      <td>301</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12504</th>\n",
              "      <td>aclImdb/train/neg/903_3.txt</td>\n",
              "      <td>903</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               path    id  score\n",
              "12500  aclImdb/train/neg/5808_4.txt  5808      4\n",
              "12501  aclImdb/train/neg/6654_1.txt  6654      1\n",
              "12502  aclImdb/train/neg/5731_1.txt  5731      1\n",
              "12503   aclImdb/train/neg/301_1.txt   301      1\n",
              "12504   aclImdb/train/neg/903_3.txt   903      3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "YivKbQrtaHMM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Logistic regression\n",
        "Here  we will have two classes, possitive and negative (1 and 0)"
      ]
    },
    {
      "metadata": {
        "id": "aQverivdaOuK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ececc60b-d9eb-4088-8694-3d7b372e75cb"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# read all \n",
        "reviews_train = []\n",
        "for i, row in pd.read_csv('imdb_train_manifest.csv').iterrows():\n",
        "   reviews_train.append(open(row['path'], \"r\").read())\n",
        "\n",
        "reviews_test = []\n",
        "for i, row in pd.read_csv('imdb_test_manifest.csv').iterrows():\n",
        "    reviews_test.append(open(row['path'], \"r\").read())\n",
        "\n",
        "    \n",
        "# clean \n",
        "REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
        "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "\n",
        "def preprocess_reviews(reviews):\n",
        "    reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n",
        "    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
        "    \n",
        "    return reviews\n",
        "\n",
        "reviews_train_clean = preprocess_reviews(reviews_train)\n",
        "reviews_test_clean = preprocess_reviews(reviews_test)\n",
        "\n",
        "# vectorize BoW, https://towardsdatascience.com/hacking-scikit-learns-vectorizers-9ef26a7170af\n",
        "cv = CountVectorizer(binary=True)\n",
        "cv.fit(reviews_train_clean)\n",
        "X = cv.transform(reviews_train_clean)\n",
        "X_test = cv.transform(reviews_test_clean)\n",
        "\n",
        "\n",
        "target = [1 if i < 12500 else 0 for i in range(25000)]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, target, train_size = 0.75\n",
        ")\n",
        "\n",
        "final_model = LogisticRegression(C=0.05)\n",
        "final_model.fit(X, target)\n",
        "print (\"Final Accuracy: %s\" % accuracy_score(target, final_model.predict(X_test)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Final Accuracy: 0.88152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x-Hx52SMmEh0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5a4e6845-0ebd-48b8-b2d2-7363496e8ab1"
      },
      "cell_type": "code",
      "source": [
        "# Let's try TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfIdf = TfidfVectorizer()\n",
        "tfIdf.fit_transform(reviews_train_clean)\n",
        "X_tfidf = tfIdf.transform(reviews_train_clean)\n",
        "X_test_tfidf = tfIdf.transform(reviews_test_clean)\n",
        "\n",
        "target = [1 if i < 12500 else 0 for i in range(25000)]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_tfidf, target, train_size = 0.75\n",
        ")\n",
        "\n",
        "# different_c = [0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.85, 1.9, 1.95, 2.1, 2.5, 3]\n",
        "# for c in different_c:\n",
        "#   model = LogisticRegression(C=c)\n",
        "#   model.fit(X, target)\n",
        "#   print(\"Accuracy: %s, %s\" % (c, accuracy_score(target, model.predict(X_test))))\n",
        "  \n",
        "final_model = LogisticRegression(C=1.85)\n",
        "final_model.fit(X, target)\n",
        "print (\"Final Accuracy : %s\" % accuracy_score(target, final_model.predict(X_test)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Final Accuracy : 0.86728\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dsmXpCiesm-Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8638c96e-ccb4-4b21-a771-7ddcd51be2a7"
      },
      "cell_type": "code",
      "source": [
        "# let't do some inference\n",
        "x = tfIdf.transform([\"It was not a nice movie\", \"It was a nice movie\", \"It was a terrible movie\", \"It was a not terrible movie\"])\n",
        "print(final_model.predict(x))\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Pc6lOSbl0PYX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As we can see negation make things complicated for sentiment analysis. Also let's try a Multiclass classifier and add  all scores from reviews"
      ]
    },
    {
      "metadata": {
        "id": "zx0_93FD0Tmt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "97dd261d-7c39-46f8-d884-377877351cb2"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "\n",
        "train_multiclass = []\n",
        "for i, row in pd.read_csv('imdb_train_manifest.csv').iterrows():\n",
        "    train_multiclass.append(row['score'])\n",
        "\n",
        "target_multiclass = []\n",
        "for i, row in pd.read_csv('imdb_test_manifest.csv').iterrows():\n",
        "    target_multiclass.append(row['score'])\n",
        "    \n",
        "X_train, X_val, y_train, y_val = train_test_split(X_tfidf, train_multiclass, train_size = 0.75)\n",
        "\n",
        "multiclass_model = OneVsRestClassifier(LogisticRegression(penalty='l2', C=6))\n",
        "multiclass_model.fit(X_train, y_train)\n",
        "print (\"Multiclass Accuracy : %s\" % accuracy_score(target_multiclass, multiclass_model.predict(X_test)))\n",
        "print (\"F1 %s \" % (f1_score(y_val, multiclass_model.predict(X_val), average=\"weighted\")))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Multiclass Accuracy : 0.35928\n",
            "F1 0.3880064735063461 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WaBUXtkOK88p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "206b2ec3-8933-49be-c104-0f4f6f767c22"
      },
      "cell_type": "code",
      "source": [
        "x = tfIdf.transform([\"It was not a nice movie\", \"It was a nice movie\", \"It was a terrible movie\", \"It was a not terrible movie\"])\n",
        "print(multiclass_model.predict(x))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[7 7 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ctuDcC2zYdri",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "2045c8d4-3139-43ba-ed3d-3af3667252d0"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_val, multiclass_model.predict(X_val)))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.54      0.80      0.65      1211\n",
            "           2       0.24      0.11      0.15       591\n",
            "           3       0.26      0.19      0.22       577\n",
            "           4       0.38      0.31      0.35       727\n",
            "           7       0.31      0.26      0.28       623\n",
            "           8       0.29      0.28      0.28       733\n",
            "           9       0.22      0.10      0.14       570\n",
            "          10       0.51      0.70      0.59      1218\n",
            "\n",
            "   micro avg       0.42      0.42      0.42      6250\n",
            "   macro avg       0.34      0.34      0.33      6250\n",
            "weighted avg       0.38      0.42      0.39      6250\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sq7_Jh-JLsEZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "3c4b511e-3afa-4c7b-af21-713b429b8dfb"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_val, multiclass_model.predict(X_val))\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1067,   15,   27,   42,    6,    9,    0,   54],\n",
              "       [ 383,   25,   27,   53,    5,   12,    1,   40],\n",
              "       [ 300,   24,   56,  130,   25,   32,    2,   54],\n",
              "       [ 240,   18,   60,  197,   54,   58,    1,   78],\n",
              "       [  44,    3,   19,   54,  151,  157,   14,  212],\n",
              "       [  43,    2,    9,   22,   90,  200,   21,  340],\n",
              "       [  27,    1,    5,   10,   32,  128,   17,  373],\n",
              "       [  74,    1,    5,   14,   40,  105,    9,  933]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "PV_-M-aNaPsR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pytorch Datasets"
      ]
    },
    {
      "metadata": {
        "id": "OnFfECNgE81B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReviewDataset(Dataset):\n",
        "  \"\"\"A pandas wrapper for CSV file \"\"\"\n",
        "\n",
        "  def __init__(self, csv_file, root_dir, transform=None):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "          csv_file (string): Path to the csv file with review score.\n",
        "          root_dir (string): Directory with all the text files. Depends where files where un-archived.\n",
        "          transform (callable, optional): Optional transform to be applied\n",
        "              on a sample.\n",
        "      \"\"\"\n",
        "      self.df = pd.read_csv(csv_file)\n",
        "      self.root_dir = root_dir\n",
        "      self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      file_name = os.path.join(self.root_dir,\n",
        "                              self.df.iloc[idx, 0])\n",
        "      f = open(file_name, \"r\")\n",
        "      text = f.read()\n",
        "      id = self.df.iloc[idx, 1]\n",
        "      score =  self.df.iloc[idx, 2]\n",
        "      sample = {'id': id, 'text': text, 'score': score}\n",
        "\n",
        "      if self.transform:\n",
        "          sample = self.transform(sample)\n",
        "\n",
        "      return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9pW4eWzoLB7N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataSet = ReviewDataset(\"imdb_train_manifest.csv\", \"/content/\")\n",
        "dataSet.__len__()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OTTsBspYMdGO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "221174ca-3494-4966-db46-808186300fed"
      },
      "cell_type": "code",
      "source": [
        "dataSet[1]"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 4093,\n",
              " 'score': 8,\n",
              " 'text': 'Regardless of what personal opinion one may have of Walerian Borowczyk grotesque yet beautiful gem \"La bête\" of 1975, one has to admit that this bizarre gem is an absolutely unique cinematic experience. Borowczyk erotic fairy tale was banned in several countries for a long time, and it is quite obvious why this controversial gem fell victim to stuporous film censors. \"La bête\" is a fascinating blend of intense and beautiful fairy-tale-like atmosphere, quite explicit eroticism and genuine weirdness that bravely refuses to take any compromise. The fact that beastiality (of sorts) is one of the film\\'s central themes did certainly not help it with the censors, but it made it highly controversial and therefore known to a wider audience.<br /><br />Pierre de l\\'Esperance (Guy Tréjan), the head of a French aristocratic family, has arranged for his somewhat demented son Mathurin (Pierre Benedetti) to marry Lucy Broadhurst (Lisbeth Hummel), the young and beautiful daughter of a wealthy English family. Due to an old curse, Mathurin\\'s uncle (Marcel Dalió) is strictly against the wedding. When Lucy and her mother arrive at the French estate, Lucy immediately gets fascinated with a portrait of the 18th century ancestor Romilda (Sirpa Lane), and with an old book depicting bizarre drawings. The story soon descends into a bizarre sexual fever-dream... Without giving away too much, I can say that fans of exceptional cinema should not consider missing this film. As bizarre as it is, \"La bête\" is doubtlessly also stunningly beautiful in style, settings and cinematography. The fever-dream-like atmosphere is present within- and out of dream-sequences. The forest estate and the imposing family mansion are magnificent settings, and the beautiful score and incredible cinematography build an overwhelming atmosphere for this grotesque tale. The very explicit sexuality ranges from erotic (elegant female nudity, ravishing actresses) to seriously demented and even somewhat disgusting (close-ups on horses\\' genitalia while having intercourse,...); in either case it is not likely to be forgotten. The entire cast of \"La bête\" is fantastic and all involved deliver great performances in eccentric characters (some of which are seriously demented). The film profits from an exceptionally beautiful cast, be it Lisbeth Hummel in the lead, Finnish actress Sirpa Lane (who sadly died of Aids in 1999) as the ancestor in the dream-sequences, or the relatively unknown but particularly ravishing actress Pascale Rivault, who plays the aristocratic daughter who takes ever opportunity to have sex with a black servant in a cupboard.<br /><br />I am intentionally not giving a full description of the most important parts of the plot as they simply have to be seen to be believed. Some scenes are among the most bizarre ever caught on film, the scenes with the eponymous \\'beast\\' definitely being among them. Certainly not everybody\\'s cup of tea, but very highly recommended to fans of controversial and unusual cinema. A true cult gem!'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "metadata": {
        "id": "kqDHUeCXOIc1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Transformations/Vectorization"
      ]
    },
    {
      "metadata": {
        "id": "x8Ep8EB0ONiE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# todo: create Transform classes\n",
        "REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
        "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "\n",
        "# stack/chain multiple transformations, lowercase -> REPLACE_NO_SPACE -> REPLACE_TAGS -> \n",
        "transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda item: [REPLACE_NO_SPACE.sub(\"\", item['text'].lower()), item['score']] ),\n",
        "    transforms.Lambda(lambda item: [REPLACE_WITH_SPACE.sub(\" \", item[0]), item[1]])\n",
        "])\n",
        "\n",
        "train = ReviewDataset(\"imdb_train_manifest.csv\", \"/content/\",  transform=transform)\n",
        "test = ReviewDataset(\"imdb_test_manifest.csv\", \"/content/\",  transform=transform)\n",
        "\n",
        "# for now vectorization is happening in train."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aJhZ220CZllh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train, batch_size=25, shuffle=True)\n",
        "test_loader = DataLoader(test, batch_size=25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9ayIAv_LZulY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for sentence, score in train_loader:\n",
        "  print(sentence[0])\n",
        "  print(score.data)\n",
        "  print(torch.ones(len(sentence[0]), dtype=torch.long) * score[0])\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUbvI4ySlXjv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For now I will try simple LSTM recurrent network but output sequence I will create and array of score of that sentence. "
      ]
    },
    {
      "metadata": {
        "id": "szdPF2G-k3_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "e6ea9793-451d-494b-80a7-9c9bf3ece558"
      },
      "cell_type": "code",
      "source": [
        "word_to_ix = {}\n",
        "for sentence in reviews_train_clean:\n",
        "    for word in sentence.split():\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "for sentence in reviews_test_clean:\n",
        "    for word in sentence.split():\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "len(word_to_ix)            "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-7cedab1b5503>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword_to_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreviews_train_clean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_to_ix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mword_to_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'reviews_train_clean' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "x1kbgA5ha5gG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o1uYN4oCgE5c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "c73fe6e3-bd90-4501-f9f2-29ad2559277e"
      },
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 32\n",
        "HIDDEN_DIM = 32\n",
        "\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), 11)\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "  \n",
        "def prepare_sequence_batch(batch_seq, to_ix):\n",
        "    idxs = [prepare_sequence(seq, to_ix) for seq in batch_seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "    \n",
        "for epoch in range(20):\n",
        "  for sentence, score in train_loader: \n",
        "    # Step 1. Remember that Pytorch accumulates gradients.\n",
        "    # We need to clear them out before each instance\n",
        "    model.zero_grad()\n",
        "    # Step 2. Get our inputs ready for the network, that is, turn them into\n",
        "    # Tensors of word indices.\n",
        "    seq = sentence[0].split()\n",
        "    sentence_in = prepare_sequence(seq, word_to_ix)\n",
        "    targets = torch.ones(len(seq), dtype=torch.long) * score[0]\n",
        "    \n",
        "    # Step 3. Run our forward pass.\n",
        "    tag_scores = model(sentence_in)\n",
        "#     print(\"Score: %s, Target len: %s, scores len: %s\" % (score[0], len(targets), len(tag_scores)))\n",
        "    # last score should be the classification class, TODO\n",
        "\n",
        "    # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "    #  calling optimizer.step()\n",
        "    loss = loss_function(tag_scores, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "  # validation loss\n",
        "  with torch.no_grad():\n",
        "    pred = []\n",
        "    real_score = []\n",
        "    losses = []\n",
        "    for sentence, score in test_loader:\n",
        "      seq = sentence[0].split()\n",
        "      inputs = prepare_sequence(seq, word_to_ix)\n",
        "      tag_scores = model(inputs)\n",
        "      pred.append(tag_scores[-1])\n",
        "      real_score.append(score)\n",
        "      targets = torch.ones(len(seq), dtype=torch.long) * score[0]\n",
        "      loss = loss_function(tag_scores, targets)\n",
        "    print(\"Test accuracy: %s\\nAverage loss %s\" % (accuracy_score(real_score, pred), np.average(losses)))"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-147-148c3d2a56e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test accuracy: %s\\nAverage loss %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and unknown targets"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "JPayLzmptufU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "KBySYO84qLoS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# prepare_sequence(\"nice work\", word_to_ix)\n",
        "# X_train[0]\n",
        "word_to_ix[\"wasn\"] = 0\n",
        "\n",
        "torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HEa4twdm7tVQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c58ef1a-2743-4fae-d2a5-18c6a39b4c2a"
      },
      "cell_type": "code",
      "source": [
        "np.average([4343.4, 3232.6])"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3788.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    }
  ]
}